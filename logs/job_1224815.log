### Starting TaskPrologue of job 1224815 on tg095 at Fri Sep 19 01:58:05 PM CEST 2025
Running on cores 8-39 with governor ondemand
Fri Sep 19 13:58:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   39C    P0             57W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Activating virtual environment at /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env
Using Python: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env/bin/python
Using pip: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env/bin/pip
Starting training script with dataset: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/data/SysMLv2_data_v1.1.csv
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Setting up trainer...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.8001, 'grad_norm': 0.22708047926425934, 'learning_rate': 9e-05, 'epoch': 0.18}
{'loss': 1.4638, 'grad_norm': 0.18104156851768494, 'learning_rate': 0.00019, 'epoch': 0.35}
{'loss': 1.3912, 'grad_norm': 0.2086198627948761, 'learning_rate': 0.00019784431137724552, 'epoch': 0.53}
{'loss': 1.2764, 'grad_norm': 0.25146469473838806, 'learning_rate': 0.00019544910179640718, 'epoch': 0.7}
{'loss': 1.21, 'grad_norm': 0.21710164844989777, 'learning_rate': 0.00019305389221556887, 'epoch': 0.88}
{'loss': 1.1051, 'grad_norm': 0.2825932204723358, 'learning_rate': 0.00019065868263473055, 'epoch': 1.05}
{'loss': 1.138, 'grad_norm': 0.2637389004230499, 'learning_rate': 0.00018826347305389224, 'epoch': 1.23}
{'loss': 1.1115, 'grad_norm': 0.32823073863983154, 'learning_rate': 0.0001858682634730539, 'epoch': 1.4}
{'loss': 1.0916, 'grad_norm': 0.2428300976753235, 'learning_rate': 0.00018347305389221558, 'epoch': 1.58}
{'loss': 1.082, 'grad_norm': 0.39662596583366394, 'learning_rate': 0.00018107784431137727, 'epoch': 1.75}
{'loss': 1.0867, 'grad_norm': 0.33519038558006287, 'learning_rate': 0.00017868263473053893, 'epoch': 1.93}
{'loss': 0.9537, 'grad_norm': 0.3038479685783386, 'learning_rate': 0.00017628742514970062, 'epoch': 2.11}
{'loss': 1.07, 'grad_norm': 0.36826545000076294, 'learning_rate': 0.00017389221556886228, 'epoch': 2.28}
{'loss': 0.9606, 'grad_norm': 0.37593382596969604, 'learning_rate': 0.00017149700598802396, 'epoch': 2.46}
{'loss': 0.9719, 'grad_norm': 0.3831121027469635, 'learning_rate': 0.00016910179640718565, 'epoch': 2.63}
{'loss': 1.0201, 'grad_norm': 0.38342979550361633, 'learning_rate': 0.00016670658682634733, 'epoch': 2.81}
{'loss': 0.9971, 'grad_norm': 0.33192089200019836, 'learning_rate': 0.00016431137724550897, 'epoch': 2.98}
{'loss': 0.9404, 'grad_norm': 0.4349425733089447, 'learning_rate': 0.00016191616766467065, 'epoch': 3.16}
{'loss': 0.9205, 'grad_norm': 0.41412192583084106, 'learning_rate': 0.00015952095808383234, 'epoch': 3.33}
{'loss': 0.914, 'grad_norm': 0.44691193103790283, 'learning_rate': 0.00015712574850299403, 'epoch': 3.51}
{'loss': 0.9455, 'grad_norm': 0.4474266469478607, 'learning_rate': 0.0001547305389221557, 'epoch': 3.68}
{'loss': 0.9141, 'grad_norm': 0.43910855054855347, 'learning_rate': 0.00015233532934131737, 'epoch': 3.86}
{'loss': 0.8717, 'grad_norm': 0.3604554235935211, 'learning_rate': 0.00014994011976047906, 'epoch': 4.04}
{'loss': 0.8572, 'grad_norm': 0.5822651386260986, 'learning_rate': 0.00014754491017964072, 'epoch': 4.21}
{'loss': 0.8316, 'grad_norm': 0.4805539846420288, 'learning_rate': 0.0001451497005988024, 'epoch': 4.39}
{'loss': 0.7951, 'grad_norm': 0.46624335646629333, 'learning_rate': 0.00014275449101796406, 'epoch': 4.56}
{'loss': 0.8951, 'grad_norm': 0.42518535256385803, 'learning_rate': 0.00014035928143712575, 'epoch': 4.74}
{'loss': 0.8823, 'grad_norm': 0.4192975163459778, 'learning_rate': 0.00013796407185628744, 'epoch': 4.91}
{'loss': 0.8077, 'grad_norm': 0.7550665736198425, 'learning_rate': 0.00013556886227544912, 'epoch': 5.09}
{'loss': 0.789, 'grad_norm': 0.6414909362792969, 'learning_rate': 0.00013317365269461078, 'epoch': 5.26}
{'loss': 0.7413, 'grad_norm': 0.6723746061325073, 'learning_rate': 0.00013077844311377244, 'epoch': 5.44}
{'loss': 0.8072, 'grad_norm': 0.5392121076583862, 'learning_rate': 0.00012838323353293413, 'epoch': 5.61}
{'loss': 0.8107, 'grad_norm': 0.51137375831604, 'learning_rate': 0.00012598802395209581, 'epoch': 5.79}
{'loss': 0.8092, 'grad_norm': 0.48971712589263916, 'learning_rate': 0.0001235928143712575, 'epoch': 5.96}
{'loss': 0.7506, 'grad_norm': 0.6184675693511963, 'learning_rate': 0.00012119760479041917, 'epoch': 6.14}
{'loss': 0.7268, 'grad_norm': 0.5013498663902283, 'learning_rate': 0.00011880239520958085, 'epoch': 6.32}
{'loss': 0.6869, 'grad_norm': 0.6203773021697998, 'learning_rate': 0.0001164071856287425, 'epoch': 6.49}
{'loss': 0.7589, 'grad_norm': 0.6674095392227173, 'learning_rate': 0.00011401197604790419, 'epoch': 6.67}
{'loss': 0.748, 'grad_norm': 0.6807904839515686, 'learning_rate': 0.00011161676646706586, 'epoch': 6.84}
{'loss': 0.7004, 'grad_norm': 0.5117546319961548, 'learning_rate': 0.00010922155688622755, 'epoch': 7.02}
{'loss': 0.6581, 'grad_norm': 0.6724045872688293, 'learning_rate': 0.00010682634730538922, 'epoch': 7.19}
{'loss': 0.6302, 'grad_norm': 0.6430041193962097, 'learning_rate': 0.00010443113772455091, 'epoch': 7.37}
{'loss': 0.6788, 'grad_norm': 0.598640501499176, 'learning_rate': 0.00010203592814371258, 'epoch': 7.54}
{'loss': 0.6807, 'grad_norm': 0.853480875492096, 'learning_rate': 9.964071856287426e-05, 'epoch': 7.72}
{'loss': 0.7509, 'grad_norm': 0.7617989778518677, 'learning_rate': 9.724550898203594e-05, 'epoch': 7.89}
{'loss': 0.611, 'grad_norm': 0.442636638879776, 'learning_rate': 9.48502994011976e-05, 'epoch': 8.07}
{'loss': 0.6488, 'grad_norm': 0.7665285468101501, 'learning_rate': 9.245508982035929e-05, 'epoch': 8.25}
{'loss': 0.6425, 'grad_norm': 0.9041233062744141, 'learning_rate': 9.005988023952096e-05, 'epoch': 8.42}
{'loss': 0.6188, 'grad_norm': 0.9237595200538635, 'learning_rate': 8.766467065868263e-05, 'epoch': 8.6}
{'loss': 0.5988, 'grad_norm': 0.8230379223823547, 'learning_rate': 8.526946107784432e-05, 'epoch': 8.77}
{'loss': 0.6544, 'grad_norm': 0.623986005783081, 'learning_rate': 8.2874251497006e-05, 'epoch': 8.95}
{'loss': 0.5744, 'grad_norm': 0.8150121569633484, 'learning_rate': 8.047904191616767e-05, 'epoch': 9.12}
{'loss': 0.5583, 'grad_norm': 0.789887547492981, 'learning_rate': 7.808383233532934e-05, 'epoch': 9.3}
{'loss': 0.5781, 'grad_norm': 0.8216833472251892, 'learning_rate': 7.568862275449103e-05, 'epoch': 9.47}
{'loss': 0.5604, 'grad_norm': 0.9404396414756775, 'learning_rate': 7.32934131736527e-05, 'epoch': 9.65}
{'loss': 0.5983, 'grad_norm': 0.9305925965309143, 'learning_rate': 7.089820359281437e-05, 'epoch': 9.82}
{'loss': 0.5955, 'grad_norm': 0.9291774034500122, 'learning_rate': 6.850299401197606e-05, 'epoch': 10.0}
{'loss': 0.5248, 'grad_norm': 0.7719616293907166, 'learning_rate': 6.610778443113773e-05, 'epoch': 10.18}
{'loss': 0.5084, 'grad_norm': 1.0455818176269531, 'learning_rate': 6.37125748502994e-05, 'epoch': 10.35}
{'loss': 0.5085, 'grad_norm': 1.0400609970092773, 'learning_rate': 6.131736526946108e-05, 'epoch': 10.53}
{'loss': 0.5481, 'grad_norm': 0.8689425587654114, 'learning_rate': 5.892215568862276e-05, 'epoch': 10.7}
{'loss': 0.5537, 'grad_norm': 0.9475670456886292, 'learning_rate': 5.652694610778443e-05, 'epoch': 10.88}
{'loss': 0.5683, 'grad_norm': 1.0161770582199097, 'learning_rate': 5.413173652694611e-05, 'epoch': 11.05}
{'loss': 0.4875, 'grad_norm': 1.063772439956665, 'learning_rate': 5.173652694610779e-05, 'epoch': 11.23}
{'loss': 0.5115, 'grad_norm': 0.7764553427696228, 'learning_rate': 4.934131736526946e-05, 'epoch': 11.4}
{'loss': 0.4918, 'grad_norm': 0.6986730098724365, 'learning_rate': 4.694610778443114e-05, 'epoch': 11.58}
{'loss': 0.471, 'grad_norm': 0.9010375142097473, 'learning_rate': 4.4550898203592814e-05, 'epoch': 11.75}
{'loss': 0.5077, 'grad_norm': 0.7058016061782837, 'learning_rate': 4.2155688622754494e-05, 'epoch': 11.93}
{'loss': 0.4979, 'grad_norm': 0.9044580459594727, 'learning_rate': 3.976047904191617e-05, 'epoch': 12.11}
{'loss': 0.4666, 'grad_norm': 0.6868816018104553, 'learning_rate': 3.7365269461077846e-05, 'epoch': 12.28}
{'loss': 0.4815, 'grad_norm': 1.1393686532974243, 'learning_rate': 3.4970059880239526e-05, 'epoch': 12.46}
{'loss': 0.4434, 'grad_norm': 0.9000757336616516, 'learning_rate': 3.25748502994012e-05, 'epoch': 12.63}
{'loss': 0.4595, 'grad_norm': 1.0125296115875244, 'learning_rate': 3.0179640718562875e-05, 'epoch': 12.81}
{'loss': 0.4649, 'grad_norm': 0.8541341423988342, 'learning_rate': 2.7784431137724555e-05, 'epoch': 12.98}
{'loss': 0.4188, 'grad_norm': 1.319669246673584, 'learning_rate': 2.5389221556886227e-05, 'epoch': 13.16}
{'loss': 0.4576, 'grad_norm': 0.9695918560028076, 'learning_rate': 2.2994011976047904e-05, 'epoch': 13.33}
{'loss': 0.4343, 'grad_norm': 0.9527466893196106, 'learning_rate': 2.0598802395209583e-05, 'epoch': 13.51}
{'loss': 0.4244, 'grad_norm': 0.8828169107437134, 'learning_rate': 1.820359281437126e-05, 'epoch': 13.68}
{'loss': 0.4401, 'grad_norm': 0.896361231803894, 'learning_rate': 1.5808383233532936e-05, 'epoch': 13.86}
{'loss': 0.4605, 'grad_norm': 0.6776599287986755, 'learning_rate': 1.341317365269461e-05, 'epoch': 14.04}
{'loss': 0.4106, 'grad_norm': 0.7828664183616638, 'learning_rate': 1.1017964071856288e-05, 'epoch': 14.21}
{'loss': 0.4245, 'grad_norm': 1.1855781078338623, 'learning_rate': 8.622754491017965e-06, 'epoch': 14.39}
{'loss': 0.4263, 'grad_norm': 0.810273289680481, 'learning_rate': 6.227544910179641e-06, 'epoch': 14.56}
{'loss': 0.428, 'grad_norm': 0.8714386224746704, 'learning_rate': 3.832335329341317e-06, 'epoch': 14.74}
{'loss': 0.4385, 'grad_norm': 1.0293099880218506, 'learning_rate': 1.437125748502994e-06, 'epoch': 14.91}
{'train_runtime': 4157.2595, 'train_samples_per_second': 3.287, 'train_steps_per_second': 0.206, 'train_loss': 0.7337424361914919, 'epoch': 15.0}
==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 404.28 out of 503.7 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Unsloth: Saving tokenizer... Done.
Done.
==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp might take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GGUF 16bits might take 3 minutes.
\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.
 "-____-"     In total, you will have to wait at least 16 minutes.

Unsloth: Installing llama.cpp. This might take 3 minutes...
Unsloth: [1] Converting model at ./SysML-V2-Qwen2.5-Coder-7B-Instruct into bf16 GGUF format.
The output location will be /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
This might take 3 minutes...
INFO:hf-to-gguf:Loading model: SysML-V2-Qwen2.5-Coder-7B-Instruct
INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'
INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'
INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {3584, 152064}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'
INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {3584, 152064}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 32768
INFO:hf-to-gguf:gguf: embedding length = 3584
INFO:hf-to-gguf:gguf: feed forward length = 18944
INFO:hf-to-gguf:gguf: head count = 28
INFO:hf-to-gguf:gguf: key-value head count = 4
INFO:hf-to-gguf:gguf: rope theta = 1000000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06
INFO:hf-to-gguf:gguf: file type = 32
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 151387 merge(s).
INFO:gguf.vocab:Setting special token type eos to 151645
INFO:gguf.vocab:Setting special token type pad to 151665
INFO:gguf.vocab:Setting special token type bos to 151643
INFO:gguf.vocab:Setting add_bos_token to False
INFO:gguf.vocab:Setting chat_template to {%- if tools %}
    {{- '<|im_start|>system\n' }}
    {%- if messages[0]['role'] == 'system' %}
        {{- messages[0]['content'] }}
    {%- else %}
        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
    {%- endif %}
    {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
    {%- for tool in tools %}
        {{- "\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
{%- else %}
    {%- if messages[0]['role'] == 'system' %}
        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
    {%- else %}
        {{- '<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n' }}
    {%- endif %}
{%- endif %}
{%- for message in messages %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
    {%- elif message.role == "assistant" %}
        {{- '<|im_start|>' + message.role }}
        {%- if message.content %}
            {{- '\n' + message.content }}
        {%- endif %}
        {%- for tool_call in message.tool_calls %}
            {%- if tool_call.function is defined %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {{- '\n<tool_call>\n{"name": "' }}
            {{- tool_call.name }}
            {{- '", "arguments": ' }}
            {{- tool_call.arguments | tojson }}
            {{- '}\n</tool_call>' }}
        {%- endfor %}
        {{- '<|im_end|>\n' }}
    {%- elif message.role == "tool" %}
        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\n<tool_response>\n' }}
        {{- message.content }}
        {{- '\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n' }}
{%- endif %}

INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf: n_tensors = 339, total_size = 15.2G
Writing:   0%|          | 0.00/15.2G [00:00<?, ?byte/s]Writing:   7%|▋         | 1.09G/15.2G [00:04<00:58, 244Mbyte/s]Writing:   8%|▊         | 1.23G/15.2G [00:05<00:58, 239Mbyte/s]Writing:   9%|▉         | 1.36G/15.2G [00:05<01:01, 227Mbyte/s]Writing:  10%|▉         | 1.50G/15.2G [00:06<00:59, 230Mbyte/s]Writing:  10%|█         | 1.53G/15.2G [00:06<01:00, 227Mbyte/s]Writing:  10%|█         | 1.55G/15.2G [00:06<01:00, 226Mbyte/s]Writing:  11%|█         | 1.69G/15.2G [00:07<00:58, 232Mbyte/s]Writing:  12%|█▏        | 1.83G/15.2G [00:07<00:56, 235Mbyte/s]Writing:  13%|█▎        | 1.96G/15.2G [00:08<00:55, 238Mbyte/s]Writing:  13%|█▎        | 1.99G/15.2G [00:08<00:57, 232Mbyte/s]Writing:  13%|█▎        | 2.02G/15.2G [00:08<00:57, 229Mbyte/s]Writing:  14%|█▍        | 2.16G/15.2G [00:09<00:54, 240Mbyte/s]Writing:  15%|█▌        | 2.29G/15.2G [00:09<00:53, 241Mbyte/s]Writing:  16%|█▌        | 2.43G/15.2G [00:10<00:53, 241Mbyte/s]Writing:  16%|█▌        | 2.46G/15.2G [00:10<00:55, 230Mbyte/s]Writing:  16%|█▋        | 2.48G/15.2G [00:10<00:56, 227Mbyte/s]Writing:  17%|█▋        | 2.62G/15.2G [00:11<00:52, 239Mbyte/s]Writing:  18%|█▊        | 2.76G/15.2G [00:11<00:51, 241Mbyte/s]Writing:  19%|█▉        | 2.90G/15.2G [00:12<00:50, 242Mbyte/s]Writing:  19%|█▉        | 2.93G/15.2G [00:12<00:53, 228Mbyte/s]Writing:  19%|█▉        | 2.95G/15.2G [00:12<00:54, 227Mbyte/s]Writing:  20%|██        | 3.09G/15.2G [00:13<00:50, 239Mbyte/s]Writing:  21%|██        | 3.23G/15.2G [00:13<00:49, 240Mbyte/s]Writing:  22%|██▏       | 3.36G/15.2G [00:14<00:49, 242Mbyte/s]Writing:  22%|██▏       | 3.39G/15.2G [00:14<00:50, 235Mbyte/s]Writing:  22%|██▏       | 3.42G/15.2G [00:14<00:51, 231Mbyte/s]Writing:  23%|██▎       | 3.56G/15.2G [00:15<00:48, 241Mbyte/s]Writing:  24%|██▍       | 3.69G/15.2G [00:15<00:47, 241Mbyte/s]Writing:  25%|██▌       | 3.83G/15.2G [00:16<00:47, 243Mbyte/s]Writing:  25%|██▌       | 3.86G/15.2G [00:16<00:48, 235Mbyte/s]Writing:  25%|██▌       | 3.88G/15.2G [00:16<00:50, 226Mbyte/s]Writing:  26%|██▋       | 4.02G/15.2G [00:17<00:47, 235Mbyte/s]Writing:  27%|██▋       | 4.16G/15.2G [00:17<00:46, 237Mbyte/s]Writing:  28%|██▊       | 4.29G/15.2G [00:18<00:46, 237Mbyte/s]Writing:  28%|██▊       | 4.32G/15.2G [00:18<00:47, 230Mbyte/s]Writing:  29%|██▊       | 4.35G/15.2G [00:18<00:47, 227Mbyte/s]Writing:  29%|██▉       | 4.49G/15.2G [00:19<00:44, 239Mbyte/s]Writing:  30%|███       | 4.62G/15.2G [00:19<00:44, 238Mbyte/s]Writing:  31%|███▏      | 4.76G/15.2G [00:20<00:43, 241Mbyte/s]Writing:  31%|███▏      | 4.79G/15.2G [00:20<00:45, 232Mbyte/s]Writing:  32%|███▏      | 4.82G/15.2G [00:20<00:45, 227Mbyte/s]Writing:  32%|███▏      | 4.85G/15.2G [00:20<00:46, 221Mbyte/s]Writing:  32%|███▏      | 4.87G/15.2G [00:20<00:47, 220Mbyte/s]Writing:  33%|███▎      | 5.01G/15.2G [00:21<00:46, 219Mbyte/s]Writing:  34%|███▍      | 5.15G/15.2G [00:21<00:44, 228Mbyte/s]Writing:  35%|███▍      | 5.29G/15.2G [00:22<00:42, 233Mbyte/s]Writing:  35%|███▍      | 5.31G/15.2G [00:22<00:44, 221Mbyte/s]Writing:  35%|███▌      | 5.34G/15.2G [00:22<00:47, 210Mbyte/s]Writing:  36%|███▌      | 5.48G/15.2G [00:23<00:43, 226Mbyte/s]Writing:  37%|███▋      | 5.62G/15.2G [00:24<00:41, 233Mbyte/s]Writing:  38%|███▊      | 5.75G/15.2G [00:24<00:40, 236Mbyte/s]Writing:  38%|███▊      | 5.78G/15.2G [00:24<00:41, 230Mbyte/s]Writing:  38%|███▊      | 5.81G/15.2G [00:24<00:41, 229Mbyte/s]Writing:  39%|███▉      | 5.95G/15.2G [00:25<00:38, 242Mbyte/s]Writing:  40%|███▉      | 6.08G/15.2G [00:25<00:37, 243Mbyte/s]Writing:  41%|████      | 6.22G/15.2G [00:26<00:36, 244Mbyte/s]Writing:  41%|████      | 6.25G/15.2G [00:26<00:38, 234Mbyte/s]Writing:  41%|████      | 6.27G/15.2G [00:26<00:38, 231Mbyte/s]Writing:  42%|████▏     | 6.41G/15.2G [00:27<00:36, 243Mbyte/s]Writing:  43%|████▎     | 6.55G/15.2G [00:27<00:35, 242Mbyte/s]Writing:  44%|████▍     | 6.68G/15.2G [00:28<00:34, 245Mbyte/s]Writing:  44%|████▍     | 6.71G/15.2G [00:28<00:35, 239Mbyte/s]Writing:  44%|████▍     | 6.74G/15.2G [00:28<00:36, 235Mbyte/s]Writing:  45%|████▌     | 6.88G/15.2G [00:29<00:34, 241Mbyte/s]Writing:  46%|████▌     | 7.01G/15.2G [00:29<00:33, 242Mbyte/s]Writing:  47%|████▋     | 7.15G/15.2G [00:30<00:33, 243Mbyte/s]Writing:  47%|████▋     | 7.18G/15.2G [00:30<00:33, 238Mbyte/s]Writing:  47%|████▋     | 7.20G/15.2G [00:30<00:34, 235Mbyte/s]Writing:  48%|████▊     | 7.34G/15.2G [00:31<00:33, 238Mbyte/s]Writing:  49%|████▉     | 7.48G/15.2G [00:31<00:32, 239Mbyte/s]Writing:  50%|████▉     | 7.62G/15.2G [00:32<00:31, 242Mbyte/s]Writing:  50%|█████     | 7.65G/15.2G [00:32<00:32, 235Mbyte/s]Writing:  50%|█████     | 7.67G/15.2G [00:32<00:32, 230Mbyte/s]Writing:  51%|█████▏    | 7.81G/15.2G [00:33<00:31, 239Mbyte/s]Writing:  52%|█████▏    | 7.95G/15.2G [00:33<00:30, 241Mbyte/s]Writing:  53%|█████▎    | 8.08G/15.2G [00:34<00:29, 242Mbyte/s]Writing:  53%|█████▎    | 8.11G/15.2G [00:34<00:30, 236Mbyte/s]Writing:  53%|█████▎    | 8.14G/15.2G [00:34<00:30, 233Mbyte/s]Writing:  54%|█████▍    | 8.28G/15.2G [00:35<00:28, 243Mbyte/s]Writing:  55%|█████▌    | 8.41G/15.2G [00:35<00:28, 243Mbyte/s]Writing:  56%|█████▌    | 8.55G/15.2G [00:36<00:27, 244Mbyte/s]Writing:  56%|█████▋    | 8.58G/15.2G [00:36<00:28, 230Mbyte/s]Writing:  56%|█████▋    | 8.60G/15.2G [00:36<00:29, 228Mbyte/s]Writing:  57%|█████▋    | 8.74G/15.2G [00:37<00:28, 231Mbyte/s]Writing:  58%|█████▊    | 8.88G/15.2G [00:37<00:26, 236Mbyte/s]Writing:  58%|█████▊    | 8.91G/15.2G [00:37<00:27, 230Mbyte/s]Writing:  59%|█████▊    | 8.93G/15.2G [00:37<00:27, 227Mbyte/s]Writing:  60%|█████▉    | 9.07G/15.2G [00:38<00:25, 237Mbyte/s]Writing:  60%|██████    | 9.21G/15.2G [00:39<00:25, 239Mbyte/s]Writing:  61%|██████▏   | 9.34G/15.2G [00:39<00:24, 241Mbyte/s]Writing:  62%|██████▏   | 9.48G/15.2G [00:40<00:23, 245Mbyte/s]Writing:  63%|██████▎   | 9.62G/15.2G [00:40<00:23, 244Mbyte/s]Writing:  64%|██████▍   | 9.75G/15.2G [00:42<00:32, 169Mbyte/s]Writing:  64%|██████▍   | 9.78G/15.2G [00:42<00:31, 171Mbyte/s]Writing:  64%|██████▍   | 9.81G/15.2G [00:42<00:31, 174Mbyte/s]Writing:  65%|██████▌   | 9.95G/15.2G [00:42<00:27, 192Mbyte/s]Writing:  66%|██████▌   | 10.1G/15.2G [00:43<00:24, 209Mbyte/s]Writing:  67%|██████▋   | 10.2G/15.2G [00:44<00:23, 213Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:44<00:21, 222Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:44<00:22, 220Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:44<00:22, 218Mbyte/s]Writing:  69%|██████▉   | 10.5G/15.2G [00:45<00:20, 231Mbyte/s]Writing:  70%|███████   | 10.7G/15.2G [00:46<00:19, 234Mbyte/s]Writing:  71%|███████   | 10.8G/15.2G [00:46<00:18, 237Mbyte/s]Writing:  71%|███████   | 10.8G/15.2G [00:46<00:19, 229Mbyte/s]Writing:  71%|███████▏  | 10.9G/15.2G [00:46<00:19, 226Mbyte/s]Writing:  72%|███████▏  | 11.0G/15.2G [00:47<00:17, 240Mbyte/s]Writing:  73%|███████▎  | 11.2G/15.2G [00:48<00:16, 242Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:48<00:16, 243Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:48<00:16, 235Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:48<00:16, 233Mbyte/s]Writing:  75%|███████▌  | 11.5G/15.2G [00:49<00:15, 240Mbyte/s]Writing:  76%|███████▋  | 11.6G/15.2G [00:50<00:14, 241Mbyte/s]Writing:  77%|███████▋  | 11.8G/15.2G [00:50<00:14, 243Mbyte/s]Writing:  77%|███████▋  | 11.8G/15.2G [00:50<00:14, 235Mbyte/s]Writing:  78%|███████▊  | 11.8G/15.2G [00:50<00:14, 233Mbyte/s]Writing:  78%|███████▊  | 11.9G/15.2G [00:51<00:13, 242Mbyte/s]Writing:  79%|███████▉  | 12.1G/15.2G [00:51<00:13, 242Mbyte/s]Writing:  80%|████████  | 12.2G/15.2G [00:52<00:12, 243Mbyte/s]Writing:  80%|████████  | 12.2G/15.2G [00:52<00:12, 236Mbyte/s]Writing:  81%|████████  | 12.3G/15.2G [00:52<00:12, 233Mbyte/s]Writing:  81%|████████▏ | 12.4G/15.2G [00:53<00:12, 234Mbyte/s]Writing:  82%|████████▏ | 12.5G/15.2G [00:53<00:11, 238Mbyte/s]Writing:  83%|████████▎ | 12.7G/15.2G [00:54<00:10, 242Mbyte/s]Writing:  83%|████████▎ | 12.7G/15.2G [00:54<00:10, 236Mbyte/s]Writing:  84%|████████▎ | 12.7G/15.2G [00:54<00:10, 233Mbyte/s]Writing:  85%|████████▍ | 12.9G/15.2G [00:55<00:09, 242Mbyte/s]Writing:  85%|████████▌ | 13.0G/15.2G [00:55<00:09, 243Mbyte/s]Writing:  86%|████████▋ | 13.2G/15.2G [00:56<00:08, 244Mbyte/s]Writing:  87%|████████▋ | 13.2G/15.2G [00:56<00:08, 237Mbyte/s]Writing:  87%|████████▋ | 13.2G/15.2G [00:56<00:08, 233Mbyte/s]Writing:  88%|████████▊ | 13.3G/15.2G [00:57<00:07, 243Mbyte/s]Writing:  89%|████████▊ | 13.5G/15.2G [00:57<00:07, 243Mbyte/s]Writing:  89%|████████▉ | 13.6G/15.2G [00:58<00:06, 244Mbyte/s]Writing:  90%|████████▉ | 13.6G/15.2G [00:58<00:06, 236Mbyte/s]Writing:  90%|████████▉ | 13.7G/15.2G [00:58<00:06, 233Mbyte/s]Writing:  91%|█████████ | 13.8G/15.2G [00:59<00:05, 242Mbyte/s]Writing:  92%|█████████▏| 13.9G/15.2G [00:59<00:05, 243Mbyte/s]Writing:  92%|█████████▏| 14.1G/15.2G [01:00<00:04, 244Mbyte/s]Writing:  93%|█████████▎| 14.1G/15.2G [01:00<00:04, 237Mbyte/s]Writing:  93%|█████████▎| 14.1G/15.2G [01:00<00:04, 233Mbyte/s]Writing: 100%|██████████| 15.2G/15.2G [01:05<00:00, 242Mbyte/s]Writing: 100%|██████████| 15.2G/15.2G [01:05<00:00, 234Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
Unsloth: Conversion completed! Output location: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...
main: build = 6188 (21c17b5b)
main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
main: quantizing '/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf' to '/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.Q4_K_M.gguf' as Q4_K_M using 256 threads
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = SysML V2 Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = SysML-V2-Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:               general.quantization_version u32              = 2
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151665
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type bf16:  198 tensors
[   1/ 339]                        output.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1039.50 MiB ->   426.36 MiB
[   2/ 339]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   3/ 339]                    token_embd.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1039.50 MiB ->   292.36 MiB
[   4/ 339]                    blk.0.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[   5/ 339]                  blk.0.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[   6/ 339]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   7/ 339]             blk.0.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[   8/ 339]                    blk.0.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   9/ 339]                  blk.0.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  10/ 339]                    blk.0.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  11/ 339]                  blk.0.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  12/ 339]                blk.0.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  13/ 339]                blk.0.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  14/ 339]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  15/ 339]                  blk.0.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  16/ 339]                    blk.1.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  17/ 339]                  blk.1.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  18/ 339]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  19/ 339]             blk.1.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  20/ 339]                    blk.1.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  21/ 339]                  blk.1.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  22/ 339]                    blk.1.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  23/ 339]                  blk.1.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  24/ 339]                blk.1.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  25/ 339]                blk.1.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  26/ 339]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  27/ 339]                  blk.1.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  28/ 339]                    blk.2.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  29/ 339]                  blk.2.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  30/ 339]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  31/ 339]             blk.2.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  32/ 339]                    blk.2.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  33/ 339]                  blk.2.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  34/ 339]                    blk.2.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  35/ 339]                  blk.2.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  36/ 339]                blk.2.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  37/ 339]                blk.2.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  38/ 339]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  39/ 339]                  blk.2.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  40/ 339]                    blk.3.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  41/ 339]                  blk.3.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  42/ 339]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  43/ 339]             blk.3.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  44/ 339]                    blk.3.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  45/ 339]                  blk.3.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  46/ 339]                    blk.3.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  47/ 339]                  blk.3.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  48/ 339]                blk.3.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  49/ 339]                blk.3.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  50/ 339]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  51/ 339]                  blk.3.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  52/ 339]                    blk.4.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  53/ 339]                  blk.4.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  54/ 339]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  55/ 339]             blk.4.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  56/ 339]                    blk.4.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  57/ 339]                  blk.4.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  58/ 339]                    blk.4.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  59/ 339]                  blk.4.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  60/ 339]                blk.4.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  61/ 339]                blk.4.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  62/ 339]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  63/ 339]                  blk.4.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  64/ 339]                    blk.5.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  65/ 339]                  blk.5.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  66/ 339]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  67/ 339]             blk.5.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  68/ 339]                    blk.5.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  69/ 339]                  blk.5.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  70/ 339]                    blk.5.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  71/ 339]                  blk.5.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  72/ 339]                blk.5.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  73/ 339]                blk.5.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  74/ 339]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  75/ 339]                  blk.5.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  76/ 339]                    blk.6.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  77/ 339]                  blk.6.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  78/ 339]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  79/ 339]             blk.6.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  80/ 339]                    blk.6.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  81/ 339]                  blk.6.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  82/ 339]                    blk.6.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  83/ 339]                  blk.6.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  84/ 339]                blk.6.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  85/ 339]                blk.6.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  86/ 339]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  87/ 339]                  blk.6.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  88/ 339]                    blk.7.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  89/ 339]                  blk.7.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  90/ 339]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  91/ 339]             blk.7.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  92/ 339]                    blk.7.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  93/ 339]                  blk.7.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  94/ 339]                    blk.7.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  95/ 339]                  blk.7.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  96/ 339]                blk.7.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  97/ 339]                blk.7.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  98/ 339]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  99/ 339]                  blk.7.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 100/ 339]                    blk.8.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 101/ 339]                  blk.8.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 102/ 339]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 103/ 339]             blk.8.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 104/ 339]                    blk.8.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 105/ 339]                  blk.8.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 106/ 339]                    blk.8.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 107/ 339]                  blk.8.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 108/ 339]                blk.8.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 109/ 339]                blk.8.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 110/ 339]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 111/ 339]                  blk.8.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 112/ 339]                    blk.9.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 113/ 339]                  blk.9.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 114/ 339]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 115/ 339]             blk.9.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 116/ 339]                    blk.9.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 117/ 339]                  blk.9.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 118/ 339]                    blk.9.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 119/ 339]                  blk.9.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 120/ 339]                blk.9.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 121/ 339]                blk.9.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 122/ 339]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 123/ 339]                  blk.9.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 124/ 339]                   blk.10.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 125/ 339]                 blk.10.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 126/ 339]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 127/ 339]            blk.10.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 128/ 339]                   blk.10.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 129/ 339]                 blk.10.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 130/ 339]                   blk.10.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 131/ 339]                 blk.10.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 132/ 339]               blk.10.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 133/ 339]               blk.10.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 134/ 339]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 135/ 339]                 blk.10.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 136/ 339]                   blk.11.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 137/ 339]                 blk.11.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 138/ 339]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 139/ 339]            blk.11.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 140/ 339]                   blk.11.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 141/ 339]                 blk.11.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 142/ 339]                   blk.11.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 143/ 339]                 blk.11.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 144/ 339]               blk.11.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 145/ 339]               blk.11.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 146/ 339]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 147/ 339]                 blk.11.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 148/ 339]                   blk.12.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 149/ 339]                 blk.12.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 150/ 339]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 151/ 339]            blk.12.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 152/ 339]                   blk.12.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 153/ 339]                 blk.12.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 154/ 339]                   blk.12.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 155/ 339]                 blk.12.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 156/ 339]               blk.12.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 157/ 339]               blk.12.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 158/ 339]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 159/ 339]                 blk.12.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 160/ 339]                   blk.13.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 161/ 339]                 blk.13.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 162/ 339]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 163/ 339]            blk.13.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 164/ 339]                   blk.13.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 165/ 339]                 blk.13.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 166/ 339]                   blk.13.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 167/ 339]                 blk.13.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 168/ 339]               blk.13.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 169/ 339]               blk.13.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 170/ 339]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 171/ 339]                 blk.13.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 172/ 339]                   blk.14.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 173/ 339]                 blk.14.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 174/ 339]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 175/ 339]            blk.14.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 176/ 339]                   blk.14.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 177/ 339]                 blk.14.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 178/ 339]                   blk.14.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 179/ 339]                 blk.14.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 180/ 339]               blk.14.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 181/ 339]               blk.14.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 182/ 339]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 183/ 339]                 blk.14.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 184/ 339]                   blk.15.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 185/ 339]                 blk.15.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 186/ 339]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 187/ 339]            blk.15.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 188/ 339]                   blk.15.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 189/ 339]                 blk.15.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 190/ 339]                   blk.15.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 191/ 339]                 blk.15.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 192/ 339]               blk.15.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 193/ 339]               blk.15.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 194/ 339]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 195/ 339]                 blk.15.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 196/ 339]                   blk.16.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 197/ 339]                 blk.16.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 198/ 339]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 199/ 339]            blk.16.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 200/ 339]                   blk.16.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 201/ 339]                 blk.16.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 202/ 339]                   blk.16.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 203/ 339]                 blk.16.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 204/ 339]               blk.16.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 205/ 339]               blk.16.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 206/ 339]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 207/ 339]                 blk.16.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 208/ 339]                   blk.17.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 209/ 339]                 blk.17.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 210/ 339]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 211/ 339]            blk.17.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 212/ 339]                   blk.17.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 213/ 339]                 blk.17.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 214/ 339]                   blk.17.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 215/ 339]                 blk.17.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 216/ 339]               blk.17.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 217/ 339]               blk.17.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 218/ 339]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 219/ 339]                 blk.17.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 220/ 339]                   blk.18.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 221/ 339]                 blk.18.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 222/ 339]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 223/ 339]            blk.18.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 224/ 339]                   blk.18.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 225/ 339]                 blk.18.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 226/ 339]                   blk.18.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 227/ 339]                 blk.18.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 228/ 339]               blk.18.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 229/ 339]               blk.18.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 230/ 339]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 231/ 339]                 blk.18.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 232/ 339]                   blk.19.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 233/ 339]                 blk.19.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 234/ 339]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 235/ 339]            blk.19.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 236/ 339]                   blk.19.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 237/ 339]                 blk.19.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 238/ 339]                   blk.19.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 239/ 339]                 blk.19.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 240/ 339]               blk.19.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 241/ 339]               blk.19.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 242/ 339]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 243/ 339]                 blk.19.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 244/ 339]                   blk.20.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 245/ 339]                 blk.20.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 246/ 339]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 247/ 339]            blk.20.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 248/ 339]                   blk.20.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 249/ 339]                 blk.20.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 250/ 339]                   blk.20.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 251/ 339]                 blk.20.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 252/ 339]               blk.20.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 253/ 339]               blk.20.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 254/ 339]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 255/ 339]                 blk.20.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 256/ 339]                   blk.21.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 257/ 339]                 blk.21.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 258/ 339]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 259/ 339]            blk.21.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 260/ 339]                   blk.21.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 261/ 339]                 blk.21.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 262/ 339]                   blk.21.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 263/ 339]                 blk.21.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 264/ 339]               blk.21.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 265/ 339]               blk.21.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 266/ 339]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 267/ 339]                 blk.21.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 268/ 339]                   blk.22.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 269/ 339]                 blk.22.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 270/ 339]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 271/ 339]            blk.22.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 272/ 339]                   blk.22.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 273/ 339]                 blk.22.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 274/ 339]                   blk.22.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 275/ 339]                 blk.22.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 276/ 339]               blk.22.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 277/ 339]               blk.22.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 278/ 339]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 279/ 339]                 blk.22.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 280/ 339]                   blk.23.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 281/ 339]                 blk.23.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 282/ 339]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 283/ 339]            blk.23.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 284/ 339]                   blk.23.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 285/ 339]                 blk.23.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 286/ 339]                   blk.23.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 287/ 339]                 blk.23.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 288/ 339]               blk.23.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 289/ 339]               blk.23.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 290/ 339]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 291/ 339]                 blk.23.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 292/ 339]                   blk.24.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 293/ 339]                 blk.24.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 294/ 339]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 295/ 339]            blk.24.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 296/ 339]                   blk.24.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 297/ 339]                 blk.24.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 298/ 339]                   blk.24.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 299/ 339]                 blk.24.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 300/ 339]               blk.24.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 301/ 339]               blk.24.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 302/ 339]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 303/ 339]                 blk.24.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 304/ 339]                   blk.25.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 305/ 339]                 blk.25.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 306/ 339]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 307/ 339]            blk.25.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 308/ 339]                   blk.25.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 309/ 339]                 blk.25.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 310/ 339]                   blk.25.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 311/ 339]                 blk.25.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 312/ 339]               blk.25.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 313/ 339]               blk.25.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 314/ 339]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 315/ 339]                 blk.25.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 316/ 339]                   blk.26.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 317/ 339]                 blk.26.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 318/ 339]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 319/ 339]            blk.26.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 320/ 339]                   blk.26.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 321/ 339]                 blk.26.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 322/ 339]                   blk.26.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 323/ 339]                 blk.26.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 324/ 339]               blk.26.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 325/ 339]               blk.26.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 326/ 339]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 327/ 339]                 blk.26.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 328/ 339]                   blk.27.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 329/ 339]                 blk.27.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 330/ 339]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 331/ 339]            blk.27.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 332/ 339]                   blk.27.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 333/ 339]                 blk.27.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 334/ 339]                   blk.27.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 335/ 339]                 blk.27.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 336/ 339]               blk.27.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 337/ 339]               blk.27.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 338/ 339]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 339/ 339]                 blk.27.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
llama_model_quantize_impl: model size  = 14526.27 MB
llama_model_quantize_impl: quant size  =  4460.45 MB

main: quantize time = 78211.51 ms
main:    total time = 78211.51 ms
Unsloth: Conversion completed! Output location: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.Q4_K_M.gguf
Job completed successfully
=== JOB_STATISTICS ===
=== current date     : Fri Sep 19 03:15:21 PM CEST 2025
= Job-ID             : 1224815 on tinygpu
= Job-Name           : llm_SE_train
= Job-Command        : /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/train_job.sh
= Initial workdir    : /home/hpc/iwi5/iwi5346h/LLM-SE_FAU
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 06:00:00
= Elapsed runtime    : 01:17:17
= Total RAM usage    : 65.2 GiB of requested  GiB (%)   
= Node list          : tg095
= Subm/Elig/Start/End: 2025-09-19T13:58:03 / 2025-09-19T13:58:03 / 2025-09-19T13:58:04 / 2025-09-19T15:15:21
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
!!! /home/hpc             167.1G   104.9G   209.7G  -29353days     116K     500K   1,000K        N/A !!!
    /home/woody           917.2M  1000.0G  1500.0G        N/A   4,552    5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:41:00.0, 680774, 88 %, 40 %, 20800 MiB, 4615289 ms
