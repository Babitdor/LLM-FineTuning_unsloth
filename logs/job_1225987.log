### Starting TaskPrologue of job 1225987 on tg097 at Sat Sep 20 04:58:48 PM CEST 2025
Running on cores 0-7,40-63 with governor ondemand
Sat Sep 20 16:58:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   43C    P0             55W /  400W |       0MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Activating virtual environment at /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env
Using Python: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env/bin/python
Using pip: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/llm-env/bin/pip
Starting training script with dataset: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/data/SysMLv2_data_v1.1.csv
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Setting up trainer...
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 1.7999, 'grad_norm': 0.2280125468969345, 'learning_rate': 9e-05, 'epoch': 0.18}
{'loss': 1.4638, 'grad_norm': 0.18271410465240479, 'learning_rate': 0.00019, 'epoch': 0.35}
{'loss': 1.391, 'grad_norm': 0.20996226370334625, 'learning_rate': 0.00019839285714285716, 'epoch': 0.53}
{'loss': 1.276, 'grad_norm': 0.2510968744754791, 'learning_rate': 0.00019660714285714285, 'epoch': 0.7}
{'loss': 1.2096, 'grad_norm': 0.21754217147827148, 'learning_rate': 0.0001948214285714286, 'epoch': 0.88}
{'loss': 1.1045, 'grad_norm': 0.2844722867012024, 'learning_rate': 0.0001930357142857143, 'epoch': 1.05}
{'loss': 1.1364, 'grad_norm': 0.26212093234062195, 'learning_rate': 0.00019125000000000001, 'epoch': 1.23}
{'loss': 1.1101, 'grad_norm': 0.27245333790779114, 'learning_rate': 0.00018946428571428573, 'epoch': 1.4}
{'loss': 1.0904, 'grad_norm': 0.24438777565956116, 'learning_rate': 0.00018767857142857144, 'epoch': 1.58}
{'loss': 1.0804, 'grad_norm': 0.4039880931377411, 'learning_rate': 0.00018589285714285715, 'epoch': 1.75}
{'loss': 1.0853, 'grad_norm': 0.33730435371398926, 'learning_rate': 0.00018410714285714287, 'epoch': 1.93}
{'loss': 0.9518, 'grad_norm': 0.31190967559814453, 'learning_rate': 0.00018232142857142858, 'epoch': 2.11}
{'loss': 1.0666, 'grad_norm': 0.3710349202156067, 'learning_rate': 0.0001805357142857143, 'epoch': 2.28}
{'loss': 0.9576, 'grad_norm': 0.379323273897171, 'learning_rate': 0.00017875, 'epoch': 2.46}
{'loss': 0.9683, 'grad_norm': 0.4093228280544281, 'learning_rate': 0.00017696428571428572, 'epoch': 2.63}
{'loss': 1.0172, 'grad_norm': 0.3865821659564972, 'learning_rate': 0.00017517857142857144, 'epoch': 2.81}
{'loss': 0.9941, 'grad_norm': 0.33024272322654724, 'learning_rate': 0.00017339285714285715, 'epoch': 2.98}
{'loss': 0.9353, 'grad_norm': 0.4389518201351166, 'learning_rate': 0.00017160714285714286, 'epoch': 3.16}
{'loss': 0.9153, 'grad_norm': 0.41018468141555786, 'learning_rate': 0.00016982142857142858, 'epoch': 3.33}
{'loss': 0.9079, 'grad_norm': 0.43418747186660767, 'learning_rate': 0.0001680357142857143, 'epoch': 3.51}
{'loss': 0.9396, 'grad_norm': 0.44204941391944885, 'learning_rate': 0.00016625000000000003, 'epoch': 3.68}
{'loss': 0.9096, 'grad_norm': 0.44089165329933167, 'learning_rate': 0.00016446428571428572, 'epoch': 3.86}
{'loss': 0.8659, 'grad_norm': 0.35723650455474854, 'learning_rate': 0.00016267857142857143, 'epoch': 4.04}
{'loss': 0.8488, 'grad_norm': 0.5754784345626831, 'learning_rate': 0.00016089285714285714, 'epoch': 4.21}
{'loss': 0.8234, 'grad_norm': 0.4838128387928009, 'learning_rate': 0.00015910714285714286, 'epoch': 4.39}
{'loss': 0.7877, 'grad_norm': 0.47407102584838867, 'learning_rate': 0.00015732142857142857, 'epoch': 4.56}
{'loss': 0.8868, 'grad_norm': 0.5593189001083374, 'learning_rate': 0.00015553571428571428, 'epoch': 4.74}
{'loss': 0.875, 'grad_norm': 0.4166930913925171, 'learning_rate': 0.00015375000000000002, 'epoch': 4.91}
{'loss': 0.7973, 'grad_norm': 0.7442374229431152, 'learning_rate': 0.0001519642857142857, 'epoch': 5.09}
{'loss': 0.7763, 'grad_norm': 0.6344904899597168, 'learning_rate': 0.00015017857142857142, 'epoch': 5.26}
{'loss': 0.7292, 'grad_norm': 0.650679349899292, 'learning_rate': 0.00014839285714285716, 'epoch': 5.44}
{'loss': 0.7952, 'grad_norm': 0.5503530502319336, 'learning_rate': 0.00014660714285714285, 'epoch': 5.61}
{'loss': 0.7992, 'grad_norm': 0.5352489948272705, 'learning_rate': 0.00014482142857142856, 'epoch': 5.79}
{'loss': 0.7983, 'grad_norm': 0.475565642118454, 'learning_rate': 0.0001430357142857143, 'epoch': 5.96}
{'loss': 0.7349, 'grad_norm': 0.5834313035011292, 'learning_rate': 0.00014125000000000002, 'epoch': 6.14}
{'loss': 0.7102, 'grad_norm': 0.5155742168426514, 'learning_rate': 0.0001394642857142857, 'epoch': 6.32}
{'loss': 0.6716, 'grad_norm': 0.6185632944107056, 'learning_rate': 0.00013767857142857145, 'epoch': 6.49}
{'loss': 0.7442, 'grad_norm': 0.6517416834831238, 'learning_rate': 0.00013589285714285716, 'epoch': 6.67}
{'loss': 0.7335, 'grad_norm': 0.6953185200691223, 'learning_rate': 0.00013410714285714284, 'epoch': 6.84}
{'loss': 0.686, 'grad_norm': 0.48927509784698486, 'learning_rate': 0.00013232142857142859, 'epoch': 7.02}
{'loss': 0.6381, 'grad_norm': 0.8481348156929016, 'learning_rate': 0.0001305357142857143, 'epoch': 7.19}
{'loss': 0.612, 'grad_norm': 0.651507556438446, 'learning_rate': 0.00012875, 'epoch': 7.37}
{'loss': 0.658, 'grad_norm': 0.5898299217224121, 'learning_rate': 0.00012696428571428573, 'epoch': 7.54}
{'loss': 0.6623, 'grad_norm': 0.8490511775016785, 'learning_rate': 0.00012517857142857144, 'epoch': 7.72}
{'loss': 0.7303, 'grad_norm': 0.755919873714447, 'learning_rate': 0.00012339285714285715, 'epoch': 7.89}
{'loss': 0.5924, 'grad_norm': 0.4520717263221741, 'learning_rate': 0.00012160714285714285, 'epoch': 8.07}
{'loss': 0.6199, 'grad_norm': 0.7194879055023193, 'learning_rate': 0.00011982142857142858, 'epoch': 8.25}
{'loss': 0.616, 'grad_norm': 0.9142073392868042, 'learning_rate': 0.0001180357142857143, 'epoch': 8.42}
{'loss': 0.5933, 'grad_norm': 0.9894784688949585, 'learning_rate': 0.00011625000000000002, 'epoch': 8.6}
{'loss': 0.5759, 'grad_norm': 0.8648049831390381, 'learning_rate': 0.0001144642857142857, 'epoch': 8.77}
{'loss': 0.6311, 'grad_norm': 0.6687209010124207, 'learning_rate': 0.00011267857142857143, 'epoch': 8.95}
{'loss': 0.5457, 'grad_norm': 0.8213880658149719, 'learning_rate': 0.00011089285714285715, 'epoch': 9.12}
{'loss': 0.5252, 'grad_norm': 0.8715960383415222, 'learning_rate': 0.00010910714285714285, 'epoch': 9.3}
{'loss': 0.5452, 'grad_norm': 0.832667887210846, 'learning_rate': 0.00010732142857142857, 'epoch': 9.47}
{'loss': 0.5291, 'grad_norm': 0.9207820296287537, 'learning_rate': 0.00010553571428571429, 'epoch': 9.65}
{'loss': 0.5675, 'grad_norm': 0.9001818299293518, 'learning_rate': 0.00010375000000000001, 'epoch': 9.82}
{'loss': 0.5655, 'grad_norm': 0.8969029188156128, 'learning_rate': 0.00010196428571428571, 'epoch': 10.0}
{'loss': 0.4829, 'grad_norm': 0.751368522644043, 'learning_rate': 0.00010017857142857143, 'epoch': 10.18}
{'loss': 0.468, 'grad_norm': 1.0795031785964966, 'learning_rate': 9.839285714285714e-05, 'epoch': 10.35}
{'loss': 0.469, 'grad_norm': 1.095534086227417, 'learning_rate': 9.660714285714287e-05, 'epoch': 10.53}
{'loss': 0.5088, 'grad_norm': 0.8382300138473511, 'learning_rate': 9.482142857142857e-05, 'epoch': 10.7}
{'loss': 0.5173, 'grad_norm': 0.9902698993682861, 'learning_rate': 9.30357142857143e-05, 'epoch': 10.88}
{'loss': 0.5251, 'grad_norm': 1.0806375741958618, 'learning_rate': 9.125e-05, 'epoch': 11.05}
{'loss': 0.4388, 'grad_norm': 1.174233317375183, 'learning_rate': 8.946428571428572e-05, 'epoch': 11.23}
{'loss': 0.4607, 'grad_norm': 0.8001428842544556, 'learning_rate': 8.767857142857144e-05, 'epoch': 11.4}
{'loss': 0.4411, 'grad_norm': 0.7418030500411987, 'learning_rate': 8.589285714285714e-05, 'epoch': 11.58}
{'loss': 0.4282, 'grad_norm': 0.990286111831665, 'learning_rate': 8.410714285714286e-05, 'epoch': 11.75}
{'loss': 0.4622, 'grad_norm': 0.7957865595817566, 'learning_rate': 8.232142857142858e-05, 'epoch': 11.93}
{'loss': 0.4385, 'grad_norm': 0.8520281910896301, 'learning_rate': 8.053571428571429e-05, 'epoch': 12.11}
{'loss': 0.4013, 'grad_norm': 0.7087230086326599, 'learning_rate': 7.875e-05, 'epoch': 12.28}
{'loss': 0.4189, 'grad_norm': 1.1509835720062256, 'learning_rate': 7.696428571428572e-05, 'epoch': 12.46}
{'loss': 0.3867, 'grad_norm': 1.123544692993164, 'learning_rate': 7.517857142857143e-05, 'epoch': 12.63}
{'loss': 0.4073, 'grad_norm': 1.1724013090133667, 'learning_rate': 7.339285714285714e-05, 'epoch': 12.81}
{'loss': 0.4099, 'grad_norm': 0.849681556224823, 'learning_rate': 7.160714285714286e-05, 'epoch': 12.98}
{'loss': 0.3467, 'grad_norm': 1.2804151773452759, 'learning_rate': 6.982142857142857e-05, 'epoch': 13.16}
{'loss': 0.3768, 'grad_norm': 1.0344401597976685, 'learning_rate': 6.80357142857143e-05, 'epoch': 13.33}
{'loss': 0.3616, 'grad_norm': 1.0109927654266357, 'learning_rate': 6.625e-05, 'epoch': 13.51}
{'loss': 0.3574, 'grad_norm': 0.9726894497871399, 'learning_rate': 6.446428571428572e-05, 'epoch': 13.68}
{'loss': 0.3717, 'grad_norm': 0.8673917055130005, 'learning_rate': 6.267857142857142e-05, 'epoch': 13.86}
{'loss': 0.3838, 'grad_norm': 0.7091885209083557, 'learning_rate': 6.089285714285714e-05, 'epoch': 14.04}
{'loss': 0.3142, 'grad_norm': 0.9295395016670227, 'learning_rate': 5.910714285714286e-05, 'epoch': 14.21}
{'loss': 0.3326, 'grad_norm': 1.2533903121948242, 'learning_rate': 5.732142857142857e-05, 'epoch': 14.39}
{'loss': 0.3333, 'grad_norm': 0.929219126701355, 'learning_rate': 5.553571428571429e-05, 'epoch': 14.56}
{'loss': 0.3397, 'grad_norm': 1.0653914213180542, 'learning_rate': 5.375e-05, 'epoch': 14.74}
{'loss': 0.3508, 'grad_norm': 1.1770055294036865, 'learning_rate': 5.196428571428572e-05, 'epoch': 14.91}
{'loss': 0.3074, 'grad_norm': 0.797067403793335, 'learning_rate': 5.017857142857143e-05, 'epoch': 15.09}
{'loss': 0.302, 'grad_norm': 1.0211273431777954, 'learning_rate': 4.8392857142857146e-05, 'epoch': 15.26}
{'loss': 0.3273, 'grad_norm': 1.1115490198135376, 'learning_rate': 4.660714285714286e-05, 'epoch': 15.44}
{'loss': 0.3131, 'grad_norm': 0.9296733736991882, 'learning_rate': 4.482142857142857e-05, 'epoch': 15.61}
{'loss': 0.2946, 'grad_norm': 0.9933419823646545, 'learning_rate': 4.303571428571429e-05, 'epoch': 15.79}
{'loss': 0.3026, 'grad_norm': 1.4458495378494263, 'learning_rate': 4.125e-05, 'epoch': 15.96}
{'loss': 0.2739, 'grad_norm': 1.0556917190551758, 'learning_rate': 3.946428571428571e-05, 'epoch': 16.14}
{'loss': 0.2846, 'grad_norm': 1.1876558065414429, 'learning_rate': 3.767857142857143e-05, 'epoch': 16.32}
{'loss': 0.2484, 'grad_norm': 1.019288182258606, 'learning_rate': 3.589285714285715e-05, 'epoch': 16.49}
{'loss': 0.2957, 'grad_norm': 1.1877572536468506, 'learning_rate': 3.410714285714286e-05, 'epoch': 16.67}
{'loss': 0.2885, 'grad_norm': 0.9388256072998047, 'learning_rate': 3.2321428571428574e-05, 'epoch': 16.84}
{'loss': 0.2748, 'grad_norm': 0.7752958536148071, 'learning_rate': 3.053571428571429e-05, 'epoch': 17.02}
{'loss': 0.2374, 'grad_norm': 0.8953903913497925, 'learning_rate': 2.8749999999999997e-05, 'epoch': 17.19}
{'loss': 0.252, 'grad_norm': 1.191631555557251, 'learning_rate': 2.6964285714285714e-05, 'epoch': 17.37}
{'loss': 0.2573, 'grad_norm': 1.2207159996032715, 'learning_rate': 2.5178571428571428e-05, 'epoch': 17.54}
{'loss': 0.2658, 'grad_norm': 1.032786250114441, 'learning_rate': 2.3392857142857145e-05, 'epoch': 17.72}
{'loss': 0.27, 'grad_norm': 1.091255784034729, 'learning_rate': 2.1607142857142858e-05, 'epoch': 17.89}
{'loss': 0.2776, 'grad_norm': 1.1500111818313599, 'learning_rate': 1.982142857142857e-05, 'epoch': 18.07}
{'loss': 0.2312, 'grad_norm': 1.0549482107162476, 'learning_rate': 1.8035714285714285e-05, 'epoch': 18.25}
{'loss': 0.2339, 'grad_norm': 0.7827621102333069, 'learning_rate': 1.6250000000000002e-05, 'epoch': 18.42}
{'loss': 0.2585, 'grad_norm': 1.2056963443756104, 'learning_rate': 1.4464285714285717e-05, 'epoch': 18.6}
{'loss': 0.2292, 'grad_norm': 0.8517229557037354, 'learning_rate': 1.2678571428571429e-05, 'epoch': 18.77}
{'loss': 0.2389, 'grad_norm': 0.8548018336296082, 'learning_rate': 1.0892857142857144e-05, 'epoch': 18.95}
{'loss': 0.2598, 'grad_norm': 0.9226800203323364, 'learning_rate': 9.107142857142858e-06, 'epoch': 19.12}
{'loss': 0.2162, 'grad_norm': 0.9643757939338684, 'learning_rate': 7.321428571428572e-06, 'epoch': 19.3}
{'loss': 0.2364, 'grad_norm': 1.2161716222763062, 'learning_rate': 5.535714285714285e-06, 'epoch': 19.47}
{'loss': 0.2277, 'grad_norm': 0.79900062084198, 'learning_rate': 3.75e-06, 'epoch': 19.65}
{'loss': 0.2325, 'grad_norm': 1.020587682723999, 'learning_rate': 1.9642857142857144e-06, 'epoch': 19.82}
{'loss': 0.2386, 'grad_norm': 1.1460307836532593, 'learning_rate': 1.7857142857142858e-07, 'epoch': 20.0}
{'train_runtime': 5494.7237, 'train_samples_per_second': 3.316, 'train_steps_per_second': 0.207, 'train_loss': 0.5946646537697106, 'epoch': 20.0}
==((====))==  Unsloth 2025.8.6: Fast Qwen2 patching. Transformers: 4.55.2.
   \\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Merging 4bit and LoRA weights to 16bit...
Unsloth: Will use up to 404.43 out of 503.7 RAM for saving.
Unsloth: Saving model... This might take 5 minutes ...
Unsloth: Saving tokenizer... Done.
Done.
==((====))==  Unsloth: Conversion from QLoRA to GGUF information
   \\   /|    [0] Installing llama.cpp might take 3 minutes.
O^O/ \_/ \    [1] Converting HF to GGUF 16bits might take 3 minutes.
\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.
 "-____-"     In total, you will have to wait at least 16 minutes.

Unsloth: Installing llama.cpp. This might take 3 minutes...
Unsloth: [1] Converting model at ./SysML-V2-Qwen2.5-Coder-7B-Instruct into bf16 GGUF format.
The output location will be /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
This might take 3 minutes...
INFO:hf-to-gguf:Loading model: SysML-V2-Qwen2.5-Coder-7B-Instruct
INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'
INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'
INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {3584, 152064}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}
INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}
INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}
INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}
INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}
INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}
INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'
INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {3584, 152064}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 32768
INFO:hf-to-gguf:gguf: embedding length = 3584
INFO:hf-to-gguf:gguf: feed forward length = 18944
INFO:hf-to-gguf:gguf: head count = 28
INFO:hf-to-gguf:gguf: key-value head count = 4
INFO:hf-to-gguf:gguf: rope theta = 1000000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06
INFO:hf-to-gguf:gguf: file type = 32
INFO:hf-to-gguf:Set model quantization version
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 151387 merge(s).
INFO:gguf.vocab:Setting special token type eos to 151645
INFO:gguf.vocab:Setting special token type pad to 151665
INFO:gguf.vocab:Setting special token type bos to 151643
INFO:gguf.vocab:Setting add_bos_token to False
INFO:gguf.vocab:Setting chat_template to {%- if tools %}
    {{- '<|im_start|>system\n' }}
    {%- if messages[0]['role'] == 'system' %}
        {{- messages[0]['content'] }}
    {%- else %}
        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
    {%- endif %}
    {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
    {%- for tool in tools %}
        {{- "\n" }}
        {{- tool | tojson }}
    {%- endfor %}
    {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
{%- else %}
    {%- if messages[0]['role'] == 'system' %}
        {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
    {%- else %}
        {{- '<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n' }}
    {%- endif %}
{%- endif %}
{%- for message in messages %}
    {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
        {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
    {%- elif message.role == "assistant" %}
        {{- '<|im_start|>' + message.role }}
        {%- if message.content %}
            {{- '\n' + message.content }}
        {%- endif %}
        {%- for tool_call in message.tool_calls %}
            {%- if tool_call.function is defined %}
                {%- set tool_call = tool_call.function %}
            {%- endif %}
            {{- '\n<tool_call>\n{"name": "' }}
            {{- tool_call.name }}
            {{- '", "arguments": ' }}
            {{- tool_call.arguments | tojson }}
            {{- '}\n</tool_call>' }}
        {%- endfor %}
        {{- '<|im_end|>\n' }}
    {%- elif message.role == "tool" %}
        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
            {{- '<|im_start|>user' }}
        {%- endif %}
        {{- '\n<tool_response>\n' }}
        {{- message.content }}
        {{- '\n</tool_response>' }}
        {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
            {{- '<|im_end|>\n' }}
        {%- endif %}
    {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
    {{- '<|im_start|>assistant\n' }}
{%- endif %}

INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf: n_tensors = 339, total_size = 15.2G
Writing:   0%|          | 0.00/15.2G [00:00<?, ?byte/s]Writing:   7%|▋         | 1.09G/15.2G [00:04<00:57, 244Mbyte/s]Writing:   8%|▊         | 1.23G/15.2G [00:05<00:58, 239Mbyte/s]Writing:   9%|▉         | 1.36G/15.2G [00:05<00:57, 240Mbyte/s]Writing:  10%|▉         | 1.50G/15.2G [00:06<00:57, 241Mbyte/s]Writing:  10%|█         | 1.53G/15.2G [00:06<00:57, 238Mbyte/s]Writing:  10%|█         | 1.55G/15.2G [00:06<00:57, 236Mbyte/s]Writing:  11%|█         | 1.69G/15.2G [00:07<00:55, 242Mbyte/s]Writing:  12%|█▏        | 1.83G/15.2G [00:07<00:55, 241Mbyte/s]Writing:  13%|█▎        | 1.96G/15.2G [00:08<00:54, 242Mbyte/s]Writing:  13%|█▎        | 1.99G/15.2G [00:08<00:55, 237Mbyte/s]Writing:  13%|█▎        | 2.02G/15.2G [00:08<00:56, 234Mbyte/s]Writing:  14%|█▍        | 2.16G/15.2G [00:08<00:53, 245Mbyte/s]Writing:  15%|█▌        | 2.29G/15.2G [00:09<00:52, 245Mbyte/s]Writing:  16%|█▌        | 2.43G/15.2G [00:10<00:52, 246Mbyte/s]Writing:  16%|█▌        | 2.46G/15.2G [00:10<00:53, 240Mbyte/s]Writing:  16%|█▋        | 2.48G/15.2G [00:10<00:53, 237Mbyte/s]Writing:  17%|█▋        | 2.62G/15.2G [00:10<00:50, 248Mbyte/s]Writing:  18%|█▊        | 2.76G/15.2G [00:11<00:50, 247Mbyte/s]Writing:  19%|█▉        | 2.90G/15.2G [00:11<00:49, 249Mbyte/s]Writing:  19%|█▉        | 2.93G/15.2G [00:12<00:51, 240Mbyte/s]Writing:  19%|█▉        | 2.95G/15.2G [00:12<00:51, 237Mbyte/s]Writing:  20%|██        | 3.09G/15.2G [00:12<00:49, 246Mbyte/s]Writing:  21%|██        | 3.23G/15.2G [00:13<00:49, 244Mbyte/s]Writing:  22%|██▏       | 3.36G/15.2G [00:13<00:48, 245Mbyte/s]Writing:  22%|██▏       | 3.39G/15.2G [00:14<00:49, 238Mbyte/s]Writing:  22%|██▏       | 3.42G/15.2G [00:14<00:50, 236Mbyte/s]Writing:  23%|██▎       | 3.56G/15.2G [00:14<00:47, 247Mbyte/s]Writing:  24%|██▍       | 3.69G/15.2G [00:15<00:47, 245Mbyte/s]Writing:  25%|██▌       | 3.83G/15.2G [00:15<00:46, 245Mbyte/s]Writing:  25%|██▌       | 3.86G/15.2G [00:15<00:47, 239Mbyte/s]Writing:  25%|██▌       | 3.88G/15.2G [00:16<00:48, 236Mbyte/s]Writing:  26%|██▋       | 4.02G/15.2G [00:16<00:45, 244Mbyte/s]Writing:  27%|██▋       | 4.16G/15.2G [00:17<00:45, 244Mbyte/s]Writing:  28%|██▊       | 4.29G/15.2G [00:17<00:44, 244Mbyte/s]Writing:  28%|██▊       | 4.32G/15.2G [00:17<00:45, 238Mbyte/s]Writing:  29%|██▊       | 4.35G/15.2G [00:18<00:46, 235Mbyte/s]Writing:  29%|██▉       | 4.49G/15.2G [00:18<00:43, 247Mbyte/s]Writing:  30%|███       | 4.62G/15.2G [00:19<00:43, 245Mbyte/s]Writing:  31%|███▏      | 4.76G/15.2G [00:19<00:42, 244Mbyte/s]Writing:  31%|███▏      | 4.79G/15.2G [00:19<00:43, 237Mbyte/s]Writing:  32%|███▏      | 4.82G/15.2G [00:19<00:44, 232Mbyte/s]Writing:  32%|███▏      | 4.85G/15.2G [00:20<00:46, 226Mbyte/s]Writing:  32%|███▏      | 4.87G/15.2G [00:20<00:46, 223Mbyte/s]Writing:  33%|███▎      | 5.01G/15.2G [00:20<00:45, 222Mbyte/s]Writing:  34%|███▍      | 5.15G/15.2G [00:21<00:43, 230Mbyte/s]Writing:  35%|███▍      | 5.29G/15.2G [00:21<00:42, 237Mbyte/s]Writing:  35%|███▍      | 5.31G/15.2G [00:22<00:42, 232Mbyte/s]Writing:  35%|███▌      | 5.34G/15.2G [00:22<00:43, 230Mbyte/s]Writing:  36%|███▌      | 5.48G/15.2G [00:22<00:40, 244Mbyte/s]Writing:  37%|███▋      | 5.62G/15.2G [00:23<00:39, 244Mbyte/s]Writing:  38%|███▊      | 5.75G/15.2G [00:23<00:38, 245Mbyte/s]Writing:  38%|███▊      | 5.78G/15.2G [00:24<00:39, 238Mbyte/s]Writing:  38%|███▊      | 5.81G/15.2G [00:24<00:39, 236Mbyte/s]Writing:  39%|███▉      | 5.95G/15.2G [00:24<00:37, 246Mbyte/s]Writing:  40%|███▉      | 6.08G/15.2G [00:25<00:37, 244Mbyte/s]Writing:  41%|████      | 6.22G/15.2G [00:25<00:36, 246Mbyte/s]Writing:  41%|████      | 6.25G/15.2G [00:25<00:37, 238Mbyte/s]Writing:  41%|████      | 6.27G/15.2G [00:26<00:38, 236Mbyte/s]Writing:  42%|████▏     | 6.41G/15.2G [00:26<00:35, 246Mbyte/s]Writing:  43%|████▎     | 6.55G/15.2G [00:27<00:35, 245Mbyte/s]Writing:  44%|████▍     | 6.68G/15.2G [00:27<00:34, 246Mbyte/s]Writing:  44%|████▍     | 6.71G/15.2G [00:27<00:35, 239Mbyte/s]Writing:  44%|████▍     | 6.74G/15.2G [00:27<00:36, 236Mbyte/s]Writing:  45%|████▌     | 6.88G/15.2G [00:28<00:33, 246Mbyte/s]Writing:  46%|████▌     | 7.01G/15.2G [00:29<00:33, 245Mbyte/s]Writing:  47%|████▋     | 7.15G/15.2G [00:29<00:32, 245Mbyte/s]Writing:  47%|████▋     | 7.18G/15.2G [00:29<00:33, 239Mbyte/s]Writing:  47%|████▋     | 7.20G/15.2G [00:29<00:34, 236Mbyte/s]Writing:  48%|████▊     | 7.34G/15.2G [00:30<00:32, 243Mbyte/s]Writing:  49%|████▉     | 7.48G/15.2G [00:31<00:31, 244Mbyte/s]Writing:  50%|████▉     | 7.62G/15.2G [00:31<00:35, 217Mbyte/s]Writing:  50%|█████     | 7.65G/15.2G [00:31<00:35, 214Mbyte/s]Writing:  50%|█████     | 7.67G/15.2G [00:32<00:35, 214Mbyte/s]Writing:  51%|█████▏    | 7.81G/15.2G [00:32<00:31, 233Mbyte/s]Writing:  52%|█████▏    | 7.95G/15.2G [00:33<00:30, 237Mbyte/s]Writing:  53%|█████▎    | 8.08G/15.2G [00:33<00:29, 241Mbyte/s]Writing:  53%|█████▎    | 8.11G/15.2G [00:33<00:30, 235Mbyte/s]Writing:  53%|█████▎    | 8.14G/15.2G [00:33<00:30, 233Mbyte/s]Writing:  54%|█████▍    | 8.28G/15.2G [00:34<00:28, 245Mbyte/s]Writing:  55%|█████▌    | 8.41G/15.2G [00:35<00:27, 244Mbyte/s]Writing:  56%|█████▌    | 8.55G/15.2G [00:35<00:27, 245Mbyte/s]Writing:  56%|█████▋    | 8.58G/15.2G [00:35<00:28, 236Mbyte/s]Writing:  56%|█████▋    | 8.60G/15.2G [00:35<00:28, 234Mbyte/s]Writing:  57%|█████▋    | 8.74G/15.2G [00:36<00:27, 240Mbyte/s]Writing:  58%|█████▊    | 8.88G/15.2G [00:37<00:26, 242Mbyte/s]Writing:  58%|█████▊    | 8.91G/15.2G [00:37<00:26, 237Mbyte/s]Writing:  59%|█████▊    | 8.93G/15.2G [00:37<00:26, 234Mbyte/s]Writing:  60%|█████▉    | 9.07G/15.2G [00:37<00:25, 244Mbyte/s]Writing:  60%|██████    | 9.21G/15.2G [00:38<00:24, 244Mbyte/s]Writing:  61%|██████▏   | 9.34G/15.2G [00:38<00:24, 245Mbyte/s]Writing:  62%|██████▏   | 9.48G/15.2G [00:39<00:23, 249Mbyte/s]Writing:  63%|██████▎   | 9.62G/15.2G [00:40<00:22, 246Mbyte/s]Writing:  64%|██████▍   | 9.75G/15.2G [00:40<00:22, 246Mbyte/s]Writing:  64%|██████▍   | 9.78G/15.2G [00:40<00:22, 241Mbyte/s]Writing:  64%|██████▍   | 9.81G/15.2G [00:40<00:22, 238Mbyte/s]Writing:  65%|██████▌   | 9.95G/15.2G [00:41<00:22, 231Mbyte/s]Writing:  66%|██████▌   | 10.1G/15.2G [00:42<00:21, 239Mbyte/s]Writing:  67%|██████▋   | 10.2G/15.2G [00:42<00:20, 240Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:43<00:20, 242Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:43<00:20, 236Mbyte/s]Writing:  68%|██████▊   | 10.4G/15.2G [00:43<00:20, 234Mbyte/s]Writing:  69%|██████▉   | 10.5G/15.2G [00:43<00:19, 243Mbyte/s]Writing:  70%|███████   | 10.7G/15.2G [00:44<00:18, 243Mbyte/s]Writing:  71%|███████   | 10.8G/15.2G [00:45<00:18, 243Mbyte/s]Writing:  71%|███████   | 10.8G/15.2G [00:45<00:18, 238Mbyte/s]Writing:  71%|███████▏  | 10.9G/15.2G [00:45<00:18, 234Mbyte/s]Writing:  72%|███████▏  | 11.0G/15.2G [00:45<00:17, 247Mbyte/s]Writing:  73%|███████▎  | 11.2G/15.2G [00:46<00:16, 245Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:46<00:16, 246Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:47<00:16, 239Mbyte/s]Writing:  74%|███████▍  | 11.3G/15.2G [00:47<00:16, 236Mbyte/s]Writing:  75%|███████▌  | 11.5G/15.2G [00:47<00:15, 246Mbyte/s]Writing:  76%|███████▋  | 11.6G/15.2G [00:48<00:14, 245Mbyte/s]Writing:  77%|███████▋  | 11.8G/15.2G [00:48<00:14, 247Mbyte/s]Writing:  77%|███████▋  | 11.8G/15.2G [00:49<00:14, 238Mbyte/s]Writing:  78%|███████▊  | 11.8G/15.2G [00:49<00:14, 235Mbyte/s]Writing:  78%|███████▊  | 11.9G/15.2G [00:49<00:13, 245Mbyte/s]Writing:  79%|███████▉  | 12.1G/15.2G [00:50<00:12, 245Mbyte/s]Writing:  80%|████████  | 12.2G/15.2G [00:50<00:12, 246Mbyte/s]Writing:  80%|████████  | 12.2G/15.2G [00:50<00:12, 240Mbyte/s]Writing:  81%|████████  | 12.3G/15.2G [00:51<00:12, 237Mbyte/s]Writing:  81%|████████▏ | 12.4G/15.2G [00:51<00:11, 243Mbyte/s]Writing:  82%|████████▏ | 12.5G/15.2G [00:52<00:11, 243Mbyte/s]Writing:  83%|████████▎ | 12.7G/15.2G [00:52<00:10, 243Mbyte/s]Writing:  83%|████████▎ | 12.7G/15.2G [00:52<00:10, 237Mbyte/s]Writing:  84%|████████▎ | 12.7G/15.2G [00:53<00:10, 234Mbyte/s]Writing:  85%|████████▍ | 12.9G/15.2G [00:53<00:09, 246Mbyte/s]Writing:  85%|████████▌ | 13.0G/15.2G [00:54<00:09, 246Mbyte/s]Writing:  86%|████████▋ | 13.2G/15.2G [00:54<00:08, 245Mbyte/s]Writing:  87%|████████▋ | 13.2G/15.2G [00:54<00:08, 238Mbyte/s]Writing:  87%|████████▋ | 13.2G/15.2G [00:54<00:08, 235Mbyte/s]Writing:  88%|████████▊ | 13.3G/15.2G [00:55<00:07, 247Mbyte/s]Writing:  89%|████████▊ | 13.5G/15.2G [00:56<00:07, 246Mbyte/s]Writing:  89%|████████▉ | 13.6G/15.2G [00:56<00:06, 247Mbyte/s]Writing:  90%|████████▉ | 13.6G/15.2G [00:56<00:06, 238Mbyte/s]Writing:  90%|████████▉ | 13.7G/15.2G [00:56<00:06, 235Mbyte/s]Writing:  91%|█████████ | 13.8G/15.2G [00:57<00:05, 247Mbyte/s]Writing:  92%|█████████▏| 13.9G/15.2G [00:57<00:05, 246Mbyte/s]Writing:  92%|█████████▏| 14.1G/15.2G [00:58<00:04, 246Mbyte/s]Writing:  93%|█████████▎| 14.1G/15.2G [00:58<00:04, 240Mbyte/s]Writing:  93%|█████████▎| 14.1G/15.2G [00:58<00:04, 238Mbyte/s]Writing: 100%|██████████| 15.2G/15.2G [01:03<00:00, 244Mbyte/s]Writing: 100%|██████████| 15.2G/15.2G [01:03<00:00, 241Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
Unsloth: Conversion completed! Output location: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf
Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...
main: build = 6188 (21c17b5b)
main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu
main: quantizing '/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf' to '/home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.Q4_K_M.gguf' as Q4_K_M using 256 threads
llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.BF16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = SysML V2 Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = SysML-V2-Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28
llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                          general.file_type u32              = 32
llama_model_loader: - kv  15:               general.quantization_version u32              = 2
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151665
llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type bf16:  198 tensors
[   1/ 339]                        output.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1039.50 MiB ->   426.36 MiB
[   2/ 339]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   3/ 339]                    token_embd.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1039.50 MiB ->   292.36 MiB
[   4/ 339]                    blk.0.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[   5/ 339]                  blk.0.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[   6/ 339]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   7/ 339]             blk.0.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[   8/ 339]                    blk.0.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[   9/ 339]                  blk.0.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  10/ 339]                    blk.0.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  11/ 339]                  blk.0.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  12/ 339]                blk.0.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  13/ 339]                blk.0.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  14/ 339]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  15/ 339]                  blk.0.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  16/ 339]                    blk.1.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  17/ 339]                  blk.1.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  18/ 339]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  19/ 339]             blk.1.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  20/ 339]                    blk.1.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  21/ 339]                  blk.1.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  22/ 339]                    blk.1.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  23/ 339]                  blk.1.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  24/ 339]                blk.1.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  25/ 339]                blk.1.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  26/ 339]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  27/ 339]                  blk.1.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  28/ 339]                    blk.2.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  29/ 339]                  blk.2.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  30/ 339]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  31/ 339]             blk.2.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  32/ 339]                    blk.2.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  33/ 339]                  blk.2.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  34/ 339]                    blk.2.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  35/ 339]                  blk.2.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  36/ 339]                blk.2.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  37/ 339]                blk.2.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  38/ 339]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  39/ 339]                  blk.2.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  40/ 339]                    blk.3.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  41/ 339]                  blk.3.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  42/ 339]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  43/ 339]             blk.3.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  44/ 339]                    blk.3.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  45/ 339]                  blk.3.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  46/ 339]                    blk.3.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  47/ 339]                  blk.3.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  48/ 339]                blk.3.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  49/ 339]                blk.3.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  50/ 339]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  51/ 339]                  blk.3.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  52/ 339]                    blk.4.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  53/ 339]                  blk.4.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  54/ 339]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  55/ 339]             blk.4.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  56/ 339]                    blk.4.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  57/ 339]                  blk.4.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  58/ 339]                    blk.4.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  59/ 339]                  blk.4.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  60/ 339]                blk.4.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  61/ 339]                blk.4.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  62/ 339]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  63/ 339]                  blk.4.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  64/ 339]                    blk.5.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  65/ 339]                  blk.5.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  66/ 339]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  67/ 339]             blk.5.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  68/ 339]                    blk.5.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  69/ 339]                  blk.5.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  70/ 339]                    blk.5.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  71/ 339]                  blk.5.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[  72/ 339]                blk.5.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[  73/ 339]                blk.5.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  74/ 339]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  75/ 339]                  blk.5.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  76/ 339]                    blk.6.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  77/ 339]                  blk.6.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  78/ 339]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  79/ 339]             blk.6.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  80/ 339]                    blk.6.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  81/ 339]                  blk.6.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  82/ 339]                    blk.6.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  83/ 339]                  blk.6.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  84/ 339]                blk.6.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  85/ 339]                blk.6.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  86/ 339]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  87/ 339]                  blk.6.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  88/ 339]                    blk.7.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  89/ 339]                  blk.7.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  90/ 339]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  91/ 339]             blk.7.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  92/ 339]                    blk.7.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  93/ 339]                  blk.7.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[  94/ 339]                    blk.7.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[  95/ 339]                  blk.7.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[  96/ 339]                blk.7.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  97/ 339]                blk.7.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[  98/ 339]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[  99/ 339]                  blk.7.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 100/ 339]                    blk.8.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 101/ 339]                  blk.8.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 102/ 339]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 103/ 339]             blk.8.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 104/ 339]                    blk.8.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 105/ 339]                  blk.8.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 106/ 339]                    blk.8.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 107/ 339]                  blk.8.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 108/ 339]                blk.8.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 109/ 339]                blk.8.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 110/ 339]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 111/ 339]                  blk.8.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 112/ 339]                    blk.9.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 113/ 339]                  blk.9.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 114/ 339]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 115/ 339]             blk.9.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 116/ 339]                    blk.9.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 117/ 339]                  blk.9.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 118/ 339]                    blk.9.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 119/ 339]                  blk.9.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 120/ 339]                blk.9.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 121/ 339]                blk.9.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 122/ 339]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 123/ 339]                  blk.9.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 124/ 339]                   blk.10.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 125/ 339]                 blk.10.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 126/ 339]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 127/ 339]            blk.10.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 128/ 339]                   blk.10.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 129/ 339]                 blk.10.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 130/ 339]                   blk.10.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 131/ 339]                 blk.10.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 132/ 339]               blk.10.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 133/ 339]               blk.10.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 134/ 339]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 135/ 339]                 blk.10.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 136/ 339]                   blk.11.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 137/ 339]                 blk.11.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 138/ 339]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 139/ 339]            blk.11.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 140/ 339]                   blk.11.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 141/ 339]                 blk.11.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 142/ 339]                   blk.11.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 143/ 339]                 blk.11.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 144/ 339]               blk.11.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 145/ 339]               blk.11.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 146/ 339]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 147/ 339]                 blk.11.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 148/ 339]                   blk.12.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 149/ 339]                 blk.12.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 150/ 339]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 151/ 339]            blk.12.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 152/ 339]                   blk.12.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 153/ 339]                 blk.12.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 154/ 339]                   blk.12.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 155/ 339]                 blk.12.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 156/ 339]               blk.12.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 157/ 339]               blk.12.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 158/ 339]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 159/ 339]                 blk.12.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 160/ 339]                   blk.13.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 161/ 339]                 blk.13.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 162/ 339]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 163/ 339]            blk.13.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 164/ 339]                   blk.13.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 165/ 339]                 blk.13.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 166/ 339]                   blk.13.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 167/ 339]                 blk.13.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 168/ 339]               blk.13.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 169/ 339]               blk.13.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 170/ 339]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 171/ 339]                 blk.13.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 172/ 339]                   blk.14.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 173/ 339]                 blk.14.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 174/ 339]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 175/ 339]            blk.14.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 176/ 339]                   blk.14.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 177/ 339]                 blk.14.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 178/ 339]                   blk.14.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 179/ 339]                 blk.14.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 180/ 339]               blk.14.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 181/ 339]               blk.14.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 182/ 339]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 183/ 339]                 blk.14.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 184/ 339]                   blk.15.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 185/ 339]                 blk.15.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 186/ 339]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 187/ 339]            blk.15.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 188/ 339]                   blk.15.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 189/ 339]                 blk.15.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 190/ 339]                   blk.15.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 191/ 339]                 blk.15.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 192/ 339]               blk.15.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 193/ 339]               blk.15.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 194/ 339]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 195/ 339]                 blk.15.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 196/ 339]                   blk.16.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 197/ 339]                 blk.16.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 198/ 339]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 199/ 339]            blk.16.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 200/ 339]                   blk.16.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 201/ 339]                 blk.16.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 202/ 339]                   blk.16.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 203/ 339]                 blk.16.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 204/ 339]               blk.16.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 205/ 339]               blk.16.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 206/ 339]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 207/ 339]                 blk.16.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 208/ 339]                   blk.17.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 209/ 339]                 blk.17.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 210/ 339]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 211/ 339]            blk.17.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 212/ 339]                   blk.17.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 213/ 339]                 blk.17.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 214/ 339]                   blk.17.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 215/ 339]                 blk.17.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 216/ 339]               blk.17.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 217/ 339]               blk.17.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 218/ 339]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 219/ 339]                 blk.17.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 220/ 339]                   blk.18.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 221/ 339]                 blk.18.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 222/ 339]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 223/ 339]            blk.18.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 224/ 339]                   blk.18.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 225/ 339]                 blk.18.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 226/ 339]                   blk.18.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 227/ 339]                 blk.18.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 228/ 339]               blk.18.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 229/ 339]               blk.18.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 230/ 339]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 231/ 339]                 blk.18.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 232/ 339]                   blk.19.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 233/ 339]                 blk.19.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 234/ 339]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 235/ 339]            blk.19.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 236/ 339]                   blk.19.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 237/ 339]                 blk.19.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 238/ 339]                   blk.19.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 239/ 339]                 blk.19.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 240/ 339]               blk.19.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 241/ 339]               blk.19.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 242/ 339]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 243/ 339]                 blk.19.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 244/ 339]                   blk.20.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 245/ 339]                 blk.20.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 246/ 339]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 247/ 339]            blk.20.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 248/ 339]                   blk.20.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 249/ 339]                 blk.20.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 250/ 339]                   blk.20.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 251/ 339]                 blk.20.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 252/ 339]               blk.20.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 253/ 339]               blk.20.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 254/ 339]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 255/ 339]                 blk.20.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 256/ 339]                   blk.21.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 257/ 339]                 blk.21.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 258/ 339]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 259/ 339]            blk.21.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 260/ 339]                   blk.21.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 261/ 339]                 blk.21.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 262/ 339]                   blk.21.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 263/ 339]                 blk.21.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 264/ 339]               blk.21.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 265/ 339]               blk.21.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 266/ 339]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 267/ 339]                 blk.21.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 268/ 339]                   blk.22.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 269/ 339]                 blk.22.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 270/ 339]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 271/ 339]            blk.22.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 272/ 339]                   blk.22.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 273/ 339]                 blk.22.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 274/ 339]                   blk.22.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 275/ 339]                 blk.22.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 276/ 339]               blk.22.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 277/ 339]               blk.22.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 278/ 339]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 279/ 339]                 blk.22.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 280/ 339]                   blk.23.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 281/ 339]                 blk.23.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 282/ 339]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 283/ 339]            blk.23.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 284/ 339]                   blk.23.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 285/ 339]                 blk.23.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 286/ 339]                   blk.23.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 287/ 339]                 blk.23.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 288/ 339]               blk.23.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 289/ 339]               blk.23.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 290/ 339]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 291/ 339]                 blk.23.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 292/ 339]                   blk.24.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 293/ 339]                 blk.24.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 294/ 339]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 295/ 339]            blk.24.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 296/ 339]                   blk.24.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 297/ 339]                 blk.24.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 298/ 339]                   blk.24.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 299/ 339]                 blk.24.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 300/ 339]               blk.24.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 301/ 339]               blk.24.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 302/ 339]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 303/ 339]                 blk.24.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 304/ 339]                   blk.25.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 305/ 339]                 blk.25.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 306/ 339]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 307/ 339]            blk.25.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 308/ 339]                   blk.25.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 309/ 339]                 blk.25.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 310/ 339]                   blk.25.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 311/ 339]                 blk.25.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 312/ 339]               blk.25.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 313/ 339]               blk.25.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 314/ 339]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 315/ 339]                 blk.25.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 316/ 339]                   blk.26.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 317/ 339]                 blk.26.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 318/ 339]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 319/ 339]            blk.26.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 320/ 339]                   blk.26.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 321/ 339]                 blk.26.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 322/ 339]                   blk.26.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 323/ 339]                 blk.26.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 324/ 339]               blk.26.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 325/ 339]               blk.26.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 326/ 339]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 327/ 339]                 blk.26.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 328/ 339]                   blk.27.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 329/ 339]                 blk.27.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB
[ 330/ 339]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 331/ 339]            blk.27.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 332/ 339]                   blk.27.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 333/ 339]                 blk.27.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB
[ 334/ 339]                   blk.27.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB
[ 335/ 339]                 blk.27.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB
[ 336/ 339]               blk.27.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB
[ 337/ 339]               blk.27.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
[ 338/ 339]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB
[ 339/ 339]                 blk.27.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB
llama_model_quantize_impl: model size  = 14526.27 MB
llama_model_quantize_impl: quant size  =  4460.45 MB

main: quantize time = 62138.23 ms
main:    total time = 62138.23 ms
Unsloth: Conversion completed! Output location: /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/SysML-V2-Qwen2.5-Coder-7B-Instruct/unsloth.Q4_K_M.gguf
Job completed successfully
=== JOB_STATISTICS ===
=== current date     : Sat Sep 20 06:37:26 PM CEST 2025
= Job-ID             : 1225987 on tinygpu
= Job-Name           : llm_SE_train
= Job-Command        : /home/hpc/iwi5/iwi5346h/LLM-SE_FAU/train_job.sh
= Initial workdir    : /home/hpc/iwi5/iwi5346h/LLM-SE_FAU
= Queue/Partition    : a100
= Slurm account      : iwi5 with QOS=normal
= Requested resources:  for 06:00:00
= Elapsed runtime    : 01:38:42
= Total RAM usage    : 63.1 GiB of requested  GiB (%)   
= Node list          : tg097
= Subm/Elig/Start/End: 2025-09-20T12:58:44 / 2025-09-20T12:58:44 / 2025-09-20T16:58:44 / 2025-09-20T18:37:26
======================
=== Quota infos ======
    Path                 Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/woody           917.2M  1000.0G  1500.0G        N/A   4,552    5,000K   7,500K        N/A    
!!! /home/hpc             171.4G   104.9G   209.7G  -29353days     116K     500K   1,000K        N/A !!!
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 1547841, 91 %, 42 %, 20800 MiB, 5899679 ms
